{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images\"\n",
    "test_path = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path=\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv\"\n",
    "df_target = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier le contenu\n",
    "df_target[:686]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_csv_path=\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv\"\n",
    "df_label = pd.read_csv(train_label_csv_path)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier le contenu\n",
    "df_label[df_label[\"study_id\"]==\"44036939\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series_csv_path=\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv\"\n",
    "df_series = pd.read_csv(train_series_csv_path)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier le contenu\n",
    "df_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv_path=\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/sample_submission.csv\"\n",
    "df = pd.read_csv(submission_csv_path)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier le contenu\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df_label=df_label[(df_label[\"study_id\"]==1524089207) & (df_label[\"series_id\"]==2221300812) & (df_label[\"instance_number\"]==12)]\n",
    "for index, row in filter_df_label.iterrows():\n",
    "    print(f\"Study ID: {row['study_id']}, Series ID: {row['series_id']}\")\n",
    "    file_name=row['condition'].replace(\" \",\"_\")+\"_\"+row['level'].replace(\"/\",\"_\")\n",
    "    print(f\"name:{file_name}\")\n",
    "    tmp_df=df_target[[\"study_id\",file_name.lower()]]\n",
    "    tmp_df=tmp_df[tmp_df[\"study_id\"]==1524089207]\n",
    "    for index, row in tmp_df.iterrows():\n",
    "        print(row[file_name.lower()])\n",
    "    print(\"\\n\")\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_file_path = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/100206310/1012284084/8.dcm\"\n",
    "\n",
    "# Lire le fichier DICOM\n",
    "dicom_data = pydicom.dcmread(dcm_file_path)\n",
    "\n",
    "# Extraire l'image du fichier DICOM\n",
    "image = dicom_data.pixel_array\n",
    "image = image.astype(np.float32)\n",
    "print(image.shape)\n",
    "image = np.squeeze(image)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('DICOM Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_dict={col:i for i, col in enumerate(df_target.columns[1:])}\n",
    "print(condition_dict)\n",
    "condition_array=[col for col in df_target.columns[1:]]\n",
    "print(condition_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "def str_is_nan(s):\n",
    "    try:\n",
    "        num = float(s)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return math.isnan(num)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.condition_labels = []\n",
    "        self.names = []\n",
    "        self.label_to_index = { 'Normal_Mild': 0, 'Moderate' : 1,    'Severe' : 2}\n",
    "        self.index_to_label = ['Normal_Mild', 'Moderate',    'Severe' ] \n",
    "        self.condition_labels_to_index = condition_dict\n",
    "        self.index_to_condition_labels = condition_array\n",
    "        self.failed_count=0\n",
    "        study_tqdm = tqdm(os.listdir(train_path), desc=f'Creation du dataset...', leave=True)\n",
    "        for study_id in study_tqdm:\n",
    "            study_folder_path = os.path.join(train_path, study_id)\n",
    "            \n",
    "            if os.path.isdir(study_folder_path):\n",
    "                # Parcourir chaque dossier de série dans le dossier study_id\n",
    "                for series_id in os.listdir(study_folder_path):\n",
    "                    series_folder_path = os.path.join(study_folder_path, series_id)\n",
    "                    \n",
    "                    if os.path.isdir(series_folder_path):\n",
    "                        # Compter le nombre de fichiers .dcm dans chaque dossier de série\n",
    "                        for f in os.listdir(series_folder_path):\n",
    "                            if f.endswith('.dcm'):\n",
    "                                filter_df_label=df_label[(df_label[\"study_id\"]==int(study_id)) & (df_label[\"series_id\"]==int(series_id)) & (df_label[\"instance_number\"]==int(f.replace(\".dcm\",\"\")))]\n",
    "                                for index, row in filter_df_label.iterrows():\n",
    "                                    file_name=row['condition'].replace(\" \",\"_\")+\"_\"+row['level'].replace(\"/\",\"_\")\n",
    "                                    file_name=file_name.lower()\n",
    "                                    tmp_df=df_target[[\"study_id\",file_name]]\n",
    "                                    tmp_df=tmp_df[tmp_df[\"study_id\"]==int(study_id)]\n",
    "                                    for index, row in tmp_df.iterrows():\n",
    "                                        key=row[file_name]\n",
    "                                        if not str_is_nan(key) :\n",
    "                                            self.names.append(study_id+\"_\"+file_name)\n",
    "                                            self.condition_labels.append(self.condition_labels_to_index[file_name])\n",
    "                                            self.files.append(os.path.join(series_folder_path, f)) \n",
    "                                            self.labels.append(self.label_to_index[key.replace(\"/\",\"_\")])\n",
    "                                        else:\n",
    "                                            self.failed_count+=1\n",
    "        if self.failed_count!=0: print(f\"There was {self.failed_count} failures\")           \n",
    "        self.transform = transform\n",
    "\n",
    "        if len(self.files) == 0:\n",
    "            raise ValueError(\"No image (dcm) files found in the specified directory.\")\n",
    "#\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        dicom_data = pydicom.dcmread(dcm_file_path)\n",
    "\n",
    "        image = dicom_data.pixel_array\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        condition = self.condition_labels[idx]\n",
    "        name=self.names[idx]\n",
    "        return image, condition,label,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(train_path, transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la classe ImageFolder assigne automatiquement un label pour chaque nom de classe (class -> idx)\n",
    "print('class -> idx : ',dataset.label_to_index)\n",
    "\n",
    "# on aura besoin d'un dictionnaire qui fait le sens inverse (idx -> class)\n",
    "idx_to_class = {dataset.label_to_index[class_name]: class_name for class_name in  dataset.label_to_index}\n",
    "print('idx -> class : ',idx_to_class)\n",
    "print(len(idx_to_class))\n",
    "\n",
    "print('class -> idx : ',dataset.condition_labels_to_index)\n",
    "\n",
    "# on aura besoin d'un dictionnaire qui fait le sens inverse (idx -> class)\n",
    "idx_to_class = {dataset.condition_labels_to_index[class_name]: class_name for class_name in  dataset.condition_labels_to_index}\n",
    "print('idx -> class : ',idx_to_class)\n",
    "print(len(idx_to_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    \"\"\"\n",
    "    Calcule la moyenne et l'écart type des canaux de l'ensemble de données d'images à une dimension.\n",
    "\n",
    "    Args:\n",
    "    dataset (torch.utils.data.Dataset): Dataset pour calculer la moyenne et l'écart type.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Moyenne et écart type des canaux de l'ensemble de données.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=1,num_workers=4, shuffle=False)\n",
    "    \n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    loader_tqdm = tqdm(loader, desc=f'Calcul des params de normalisation...', leave=True)\n",
    "    for data in loader_tqdm :\n",
    "        inputs, c,_,n = data  # Extraction des données et étiquettes\n",
    "        inputs = inputs.squeeze()  # Suppression de la dimension du lot\n",
    "        channels_sum += torch.mean(inputs, dim=0)\n",
    "        channels_squared_sum += torch.mean(inputs**2, dim=0)\n",
    "        num_batches += 1\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean**2)**0.5\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,std = get_mean_and_std(dataset)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std[std == 0] = 1e-8 \n",
    "mean[mean == 0] = 1e-8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des indices pour l'entraînement, la validation et le test\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "num_train = int(0.7 * len(indices))\n",
    "num_val = int(0.2 * len(indices))\n",
    "\n",
    "train_indices = indices[:num_train]\n",
    "val_indices = indices[num_train:num_train + num_val]\n",
    "test_indices = indices[num_train + num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TransformingSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        \"\"\"\n",
    "        subset: Un objet Subset de PyTorch contenant les indices et le dataset original.\n",
    "        transform: Les transformations à appliquer aux éléments du dataset.\n",
    "        \"\"\"\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Récupérer l'élément depuis le subset\n",
    "        x, y,z,w = self.subset[index]\n",
    "\n",
    "        # Appliquer les transformations\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y,z,w\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from albumentations.core.composition import BboxParams, KeypointParams\n",
    "class AlbumentationsAdapter:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        return self.transform(image=img)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, mask_size, p=0.5):\n",
    "        self.mask_size = mask_size\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.rand() > self.p:\n",
    "            return img\n",
    "\n",
    "        h, w = img.size()[1], img.size()[2]\n",
    "        mask_size_half = self.mask_size // 2\n",
    "\n",
    "        cxmin, cxmax = mask_size_half, w - mask_size_half\n",
    "        cymin, cymax = mask_size_half, h - mask_size_half\n",
    "\n",
    "        if cxmin >= cxmax or cymin >= cymax:\n",
    "            return img\n",
    "        \n",
    "        cx = np.random.randint(cxmin, cxmax)\n",
    "        cy = np.random.randint(cymin, cymax)\n",
    "\n",
    "        xmin, xmax = cx - mask_size_half, cx + mask_size_half\n",
    "        ymin, ymax = cy - mask_size_half, cy + mask_size_half\n",
    "\n",
    "        img[:, ymin:ymax, xmin:xmax] = 0\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transform_v2 = AlbumentationsAdapter(A.Compose([\n",
    "    # Assurez-vous que les dimensions sont appropriées pour vos spectrogrammes si nécessaire\n",
    "    #transforms.ToPILImage(), \n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    ToTensorV2(),\n",
    "    #A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    #A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    \n",
    "    \n",
    "]))\n",
    "\n",
    "train_transform_v3 = AlbumentationsAdapter(A.Compose([\n",
    "    # Assurez-vous que les dimensions sont appropriées pour vos spectrogrammes si nécessaire\n",
    "    #transforms.ToPILImage(), \n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    ToTensorV2(),\n",
    "    #A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=40, p=0.5),\n",
    "    A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
    "    A.GridDistortion(p=0.5),\n",
    "    A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=7, p=0.5),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.ChannelShuffle(p=0.5),\n",
    "    A.CLAHE(clip_limit=2, p=0.5),\n",
    "    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, brightness_coeff=2.5, p=0.5),\n",
    "    A.RandomRain(drop_length=5, drop_width=1, rain_type='drizzle', p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    \n",
    "]))\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.Grayscale(num_output_channels=3),\n",
    "   \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean.unsqueeze(0), std.unsqueeze(0)),\n",
    "     Cutout(mask_size=40, p=0.5),\n",
    "    Cutout(mask_size=20, p=0.5),\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),  # Flip horizontal aléatoire\n",
    "    transforms.RandomVerticalFlip(),  # Flip vertical aléatoire\n",
    "    transforms.RandomRotation(30),  # Rotation aléatoire jusqu'à 30 degrés\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Ajustements aléatoires des couleurs\n",
    "    #transforms.RandomResizedCrop(224),  # Recadrage aléatoire avec redimensionnement à 224x224\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)),  # Transformation affine aléatoire\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),  # Perspective aléatoire\n",
    "    transforms.RandomGrayscale(p=0.1),  # Convertir en niveaux de gris aléatoirement\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),  # Flou gaussien aléatoire\n",
    "  # Convertir en Tensor\n",
    "   # Normalisation avec les valeurs moyennes et std de ImageNet\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "     #transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean.unsqueeze(0), std.unsqueeze(0)),\n",
    "    #transforms.Resize((224, 224)),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "# Créer les instances de TransformingSubset\n",
    "train_dataset = TransformingSubset(train_subset, transform=train_transform)\n",
    "\n",
    "valid_dataset = TransformingSubset(val_subset, transform=test_transform)\n",
    "custum_test_dataset = TransformingSubset(test_subset, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # Compter le nombre d'images par classe\n",
    "# counter = Counter(train_dataset.labels)\n",
    "# classes = list(counter.keys())\n",
    "# counts = list(counter.values())\n",
    "\n",
    "# # Visualiser la distribution des classes\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.bar(classes, counts, color='blue')\n",
    "# plt.xlabel('Classe')\n",
    "# plt.ylabel('Nombre d\\'images')\n",
    "# plt.title('Distribution des images par classe')\n",
    "# plt.xticks(classes)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # Compter le nombre d'images par classe\n",
    "# counter = Counter(dataset_balance.balanced_labels)\n",
    "# classes = list(counter.keys())\n",
    "# counts = list(counter.values())\n",
    "\n",
    "# # Visualiser la distribution des classes\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.bar(classes, counts, color='blue')\n",
    "# plt.xlabel('Classe')\n",
    "# plt.ylabel('Nombre d\\'images')\n",
    "# plt.title('Distribution des images par classe')\n",
    "# plt.xticks(classes)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "for x,y,z,n in train_dataset:\n",
    "    print(y) # afficher le nom de l'image   \n",
    "    print(z) # afficher le nom de l'image\n",
    "    print(x) # afficher le nom de l'image\n",
    "\n",
    "  # Afficher le spectrogramme\n",
    "    #print(x.shape)\n",
    "    plt.imshow(x.permute(1, 2, 0), cmap='gray')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f\"Spectrogram Label: {dataset.index_to_label[z]}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset),len(valid_dataset),len(custum_test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
